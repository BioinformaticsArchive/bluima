3.3 Latent Dirichlet Allocation
While the pLSA represents a significant step toward
probabilistic modeling of text, it is incomplete in that it
provides no probabilistic model at the level of documents
[12]. In other words, the model does not have any control
over how the mixture weights p(z|d) are generated. The
limitation leads to two problems: the number of parameters
that need to be estimated grows linearly with the size of the
corpus, which leads to overfitting [12]; and it is difficult to
test generalizability of the model to new documents [14].
The Latent Dirichlet Allocation [12] is an attempt to
improve the pLSA by introducing a Dirichlet prior on
document-topic distribution. As a conjugate prior for multinomial distributions [24], Dirichlet prior simplifies the
problem of statistical inference. Steyvers and Griffiths
explore a variant of the LDA by placing a symmetric Dirichlet
prior on topic-word distribution [14], [13], and demonstrate
how to perform parameter estimation using Gibbs sampling,
a form of the Markov Chain Monte Carlo [25].
The generative process in the LDA is similar to that in
the pLSA: each word w in a document d is generated by
sampling a topic z from topic distribution, and then
