[23] J. Bilmes, “A Gentle Tutorial on the Em Algorithm and Its
Application to Parameter Estimation for Gaussian Mixture and
Hidden Markov Models,” Technical Report ICSI-TR-97-021, Univ.
of Berkeley, 1997.
[24] G. D'Agostini, “Bayesian Inference in Processing Experimental
Data: Principles and Basic Applications,” Reports on Progress in
Physics, vol. 66, no. 9, pp. 1383-1419, 2003.
[25] C. Andrieu, N. de Freitas, A. Doucet, and M.I. Jordan, “An
Introduction to MCMC for Machine Learning,” Machine Learning,
vol. 50, nos. 1/2, pp. 5-43, 2003.
[26] T. Griffiths, “Gibbs Sampling in the Generative Model of Latent
Dirichlet Allocation,” technical report, Stanford Univ., 2002.
[27] D.J. MacKay, Information Theory, Inference, and Learning Algorithms.
Cambridge Univ. Press, 2003.
[28] E. Zavitsanos, G. Paliouras, G.A. Vouros, and S. Petridis,
“Discovering Subsumption Hierarchies of Ontology Concepts
from Text Corpora,” Proc. IEEE/WIC/ACM Int'l Conf. Web
Intelligence (WI '07), pp. 402-408, 2007.
[29] L. Itti and P. Baldi, “Bayesian Surprise Attracts Human Attention,” Advances in Neural Information Processing Systems, vol. 19,
pp. 547-554, MIT Press, 2006.
[30] C.D. Manning, P. Raghavan, and H. Schu ̈ tze, Introduction to
Information Retrieval. Cambridge Univ. Press, 2008.
